# ğŸš€ sql2data Bulk Demo: PostgreSQL â¡ï¸ Parquet â¡ï¸ Delta Lake via Spark + MinIO

This demo showcases a full data pipeline using `sql2data`, converting PostgreSQL data into Parquet, storing it on MinIO, and transforming it into Delta Lake format via Spark.

---

## ğŸ§± Components

- **PostgreSQL** â€“ Seeded with 3 million `sales` records.
- **MinIO** â€“ S3-compatible storage for staging Parquet files.
- **sql2data** â€“ Exports data to partitioned or flat Parquet.
- **Apache Spark + Delta Lake** â€“ Converts Parquet to Delta Lake format.

---

## ğŸ§¹ Optional: Reset State

To delete old outputs and demo data:

```bash
docker compose down -v
rm -rf delta_output/ sales_delta/ sales_partitioned_delta/
rm -f sales_delta.parquet sales_partitioned_delta.parquet sales.parquet *.db
docker rm demo-db-spark-minio-postgres demo-minio
docker volume rm demo_minio-data spark_delta_pgdata
```

---

## â–¶ï¸ Run the Demo

The demo supports **multiple execution modes**. Hereâ€™s what each option does:

---

### âœ… `./run_sql2data.sh`

- Uses flat mode (no partitioning)
- Exports Parquet to: `sales_delta.parquet`
- Uploads to MinIO at: `sales_delta/sales_delta.parquet`
- Spark reads it and writes unpartitioned Delta to: `delta_output/`

---

### âœ… `./run_sql2data_bulk.sh`

#### ğŸ”¹ No options

- Uses default values:
  - Partitioned: âŒ
  - Output dir: `sales_delta`
- Effectively behaves like `./run_sql2data.sh` (flat mode)

---

### âœ… `./run_sql2data_bulk.sh --partitioned`

- Enables partitioning by `region`
- Output dir defaults to: `sales_delta/`
- Output structure:
  ```
  sales_delta/
    â””â”€â”€ data/
        â”œâ”€â”€ region=EMEA/
        â”œâ”€â”€ region=NA/
        â””â”€â”€ region=APAC/
  ```
- Spark reads partitioned Parquet and writes Delta partitioned by `region`

---

### âœ… `./run_sql2data_bulk.sh --output-dir sales_partitioned_delta`

- Uses flat mode (no `--partitioned` specified)
- Exports single Parquet file to: `sales_partitioned_delta.parquet`
- Spark reads and writes non-partitioned Delta to: `delta_output/`

---

### âœ… `./run_sql2data_bulk.sh --partitioned --output-dir sales_partitioned_delta`

- **Recommended** for bulk mode testing
- Enables region partitioning
- Output dir is explicitly `sales_partitioned_delta/`
- Produces a partitioned Parquet directory and corresponding partitioned Delta table

---

## ğŸ” Previewing the Result

Use DuckDB to preview partitioned output recursively:

```bash
duckdb -c "SELECT COUNT(*) FROM 'sales_partitioned_delta/**/*.parquet';"
```

> Note: DuckDB doesnâ€™t support Delta metadata, so we preview raw Parquet files.

---

## ğŸ“‚ Files Involved

- `run_sql2data_bulk.sh` â€“ Orchestrates both flat and partitioned bulk pipelines.
- `run_sql2data.sh` â€“ Simpler flat-mode demo.
- `run_spark_query.py` â€“ Runs inside Spark container to convert Parquet â Delta.
- `docker-compose.yml` â€“ Sets up PostgreSQL, MinIO, and Spark.
