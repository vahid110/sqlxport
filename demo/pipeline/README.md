# ğŸ› ï¸ sql2data Pipeline Demo (Phase 1)

This demo shows a realistic but minimal ETL pipeline built with **bash scripts** using `sql2data`, DuckDB, and optionally MinIO/S3 as the target.

---

## ğŸ¯ Goal

Simulate a production-like ETL pipeline that:
- Extracts data from PostgreSQL
- Transforms and optionally partitions it (Parquet/CSV)
- Validates and previews output
- Stores to local or S3-compatible bucket

---

## ğŸ“¦ Requirements

- Docker + Docker Compose
- Python 3 (`sql2data`, `pandas`, `duckdb`)
- Optional: MinIO or real S3 credentials

---

## ğŸš¦ How to Run

```bash
./run_pipeline.sh [--partitioned] [--use-s3] [--format csv|parquet] [--output-dir my_output]
```

### Examples

- Local, flat Parquet:
  ```bash
  ./run_pipeline.sh
  ```

- Local, partitioned Parquet:
  ```bash
  ./run_pipeline.sh --partitioned
  ```

- Upload to S3/MinIO:
  ```bash
  ./run_pipeline.sh --use-s3 --partitioned --format parquet --output-dir sales_data
  ```

---

## ğŸ”„ Stages in the Pipeline

| Stage     | Description                            | Tool         |
|-----------|----------------------------------------|--------------|
| Extract   | Run SQL query and fetch results        | sql2data     |
| Transform | Save to Parquet/CSV                    | sql2data     |
| Validate  | Check row count and preview sample     | DuckDB/Pandas|
| Load      | Store locally or to S3/MinIO           | sql2data/mc  |

---

## ğŸ§ª Output

Output files are saved in:
- Local: `pipeline_output/` or the `--output-dir` you provide
- S3: Under `s3://<bucket>/<key>/`

Partitioned outputs follow this structure:
```
pipeline_output/
  â”œâ”€â”€ region=NA/
  â”œâ”€â”€ region=EMEA/
  â””â”€â”€ region=APAC/
```

---

## ğŸ§¹ Cleanup

```bash
docker compose down -v
rm -rf pipeline_output/
```

---

## ğŸ““ Optional Preview

### With DuckDB
```bash
duckdb -c "SELECT COUNT(*) FROM 'pipeline_output/*.parquet'"
```

### With Jupyter Notebook
```bash
jupyter notebook preview.ipynb
```

---

## ğŸ“ Files

- `run_pipeline.sh` â€” main driver script
- `extract_config.sh` â€” DB URL, SQL query, etc.
- `validate_output.sh` â€” data validators (row count, schema)
- `preview.ipynb` â€” optional visual preview

---

## ğŸš€ Next Phases

- Phase 2: Replace `bash` with Python orchestration
- Phase 3: Add Apache Airflow

